{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM+WQ0NSgx9BKN1OmSlZ+i1"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":928025,"sourceType":"datasetVersion","datasetId":500970}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸŽµ Urban Sound Classification using Deep Learning\n\n**Project:** CSC4025Z Assignment 2 - Neural Network Application  \n**Dataset:** Urban Sound 8K (8,732 audio samples, 10 classes)  \n**Task:** Multi-class audio classification  \n\n---\n\n## Project Overview\n\nThis notebook implements and compares two approaches for urban sound classification:\n\n1. **Baseline:** Naive Bayes Classifier\n2. **Neural Network:** Artificial Neural Network (ANN)\n\n---\n\n## Dataset: Urban Sound 8K\n\n**Classes (10):**\n1. air_conditioner\n2. car_horn\n3. children_playing\n4. dog_bark\n5. drilling\n6. engine_idling\n7. gun_shot\n8. jackhammer\n9. siren\n10. street_music\n\n**Samples:** 8,732 audio files (4 seconds each)  ","metadata":{"id":"XOX4Gxs3m0mM"}},{"cell_type":"markdown","source":"---\n# Setting Up\n---","metadata":{"id":"uO1DgIIdnsoE"}},{"cell_type":"markdown","source":"### SYSTEM INFORMATION","metadata":{"id":"AXsbZMp0u7TN"}},{"cell_type":"code","source":"import torch\nimport sys\n\n# Check Python version\nprint(f\"Python version: {sys.version}\")\n\n# Check PyTorch version\nprint(f\"PyTorch version: {torch.__version__}\")\n\n# Check CUDA availability\nif torch.cuda.is_available():\n    print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA Version: {torch.version.cuda}\")\n    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n    device = torch.device('cuda')\nelse:\n    print(\"WARNING: GPU not available, using CPU\")\n    device = torch.device('cpu')\n\nprint(f\"\\nUsing device: {device}\")","metadata":{"id":"Q9qnMyAZnTo9","executionInfo":{"status":"ok","timestamp":1761301345661,"user_tz":-120,"elapsed":1776,"user":{"displayName":"Lutho Mngqibisa","userId":"08014628584478494004"}},"outputId":"db5baaf0-1d71-47c5-d068-a46bcb04dddd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nPROJECT_DIR = '/kaggle/working/UrbanSound_Project'\nos.makedirs(PROJECT_DIR, exist_ok=True)\n\n# Create subdirectories\nos.makedirs(f'{PROJECT_DIR}/models', exist_ok=True)\nos.makedirs(f'{PROJECT_DIR}/results', exist_ok=True)\nos.makedirs(f'{PROJECT_DIR}/figures', exist_ok=True)\n\nprint(f\"Project directory: {PROJECT_DIR}\")\nprint(\"Subdirectories created\")","metadata":{"id":"hXgCBMiDpP29","executionInfo":{"status":"ok","timestamp":1761301356907,"user_tz":-120,"elapsed":1896,"user":{"displayName":"Lutho Mngqibisa","userId":"08014628584478494004"}},"outputId":"25f4fda2-6a26-431e-93e8-46d873e3cd3b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### IMPORTING LIBRARIES","metadata":{"id":"EOvSeEzeu2f6"}},{"cell_type":"code","source":"# Standard libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport warnings\nimport json\nimport pickle\nimport time\nfrom pathlib import Path\n\n# Audio processing\nimport librosa\nimport librosa.display\nimport soundfile as sf\nimport IPython.display as ipd\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    classification_report,\n    confusion_matrix\n)\n\n# Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n\n# Seting random seeds for reproducibility\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(RANDOM_SEED)\n    torch.cuda.manual_seed_all(RANDOM_SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Plotting settings\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\nwarnings.filterwarnings('ignore')\n\n# Display settings\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\n\nprint(\"All libraries imported successfully!\")\nprint(f\"Random seed set to: {RANDOM_SEED}\")\nprint(f\"Device: {device}\")","metadata":{"id":"8kbkRYNMqBNg","executionInfo":{"status":"ok","timestamp":1761301358100,"user_tz":-120,"elapsed":1178,"user":{"displayName":"Lutho Mngqibisa","userId":"08014628584478494004"}},"outputId":"1585e185-e036-471b-b8d6-1d9c1d58e633","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### GLOBAL CONFIGURATION","metadata":{"id":"ZRK_qLv4uxXI"}},{"cell_type":"code","source":"# Configuration dictionary\nCONFIG = {\n    'project_dir': PROJECT_DIR,\n    'data_dir': '/kaggle/input/urbansound8k',  \n    'base_dir': '/kaggle/input/urbansound8k',\n    'audio_dir': '/kaggle/input/urbansound8k',\n    'metadata_path': '/kaggle/input/urbansound8k/UrbanSound8K.csv',\n    'models_dir': f'{PROJECT_DIR}/models',\n    'results_dir': f'{PROJECT_DIR}/results',\n    'figures_dir': f'{PROJECT_DIR}/figures',\n\n    # Audio parameters\n    'sample_rate': 22050,\n    'duration': 4,\n    'n_mfcc': 120,  # Based on Barua et al. (2023)\n\n    # Dataset parameters\n    'n_classes': 10,\n    'class_names': [\n        'air_conditioner',\n        'car_horn',\n        'children_playing',\n        'dog_bark',\n        'drilling',\n        'engine_idling',\n        'gun_shot',\n        'jackhammer',\n        'siren',\n        'street_music'\n    ],\n\n    # Training parameters\n    'train_split': 0.70,\n    'val_split': 0.10,\n    'test_split': 0.20,\n    'batch_size': 400,  # Based on Barua et al. (2023)\n    'learning_rate': 0.001, #Start value\n    'num_epochs': 100,  # Start value\n    'early_stopping_patience': 20,\n\n    # Model architecture (Based on Barua et al. 2023)\n    'hidden_layers': [1000, 750, 500, 250, 100],\n    'dropout_rate': 0.5,\n\n    # Random seed\n    'random_seed': RANDOM_SEED,\n    'device': str(device)\n}\n\n# Save configuration\nwith open(f\"{CONFIG['results_dir']}/config.json\", 'w') as f:\n    json.dump(CONFIG, f, indent=4)\n\n# Display configuration\nprint(\"\\nConfiguration:\")\nfor key, value in CONFIG.items():\n    if key != 'class_names':  # Skip long list\n        print(f\"   {key}: {value}\")\n    else:\n        print(f\"   {key}: {len(value)} classes\")\n\nprint(f\"\\nConfiguration saved to: {CONFIG['results_dir']}/config.json\")","metadata":{"id":"UJmUFzdVqq74","executionInfo":{"status":"ok","timestamp":1761301358179,"user_tz":-120,"elapsed":42,"user":{"displayName":"Lutho Mngqibisa","userId":"08014628584478494004"}},"outputId":"7db19704-5c25-4fac-e20a-c49aa8046f13","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n# Dataset Loading & Exploration\n\n---","metadata":{"id":"sl3UMyAutPLn"}},{"cell_type":"markdown","source":"### VERIFYING DATASET SETUP","metadata":{"id":"HpI208plBYzE"}},{"cell_type":"code","source":"# Check dataset location\ndataset_path = CONFIG['base_dir']\nmetadata_path = CONFIG['metadata_path']\n\n# Check folds\nif os.path.exists(dataset_path):\n    items = os.listdir(dataset_path)\n    folds = sorted([f for f in items if f.startswith('fold') and os.path.isdir(f\"{dataset_path}/{f}\")])\n    \n    print(f\"\\nFound {len(folds)} fold directories:\")\n    \n    total_files = 0\n    for fold in folds:\n        fold_path = os.path.join(dataset_path, fold)\n        wav_files = [f for f in os.listdir(fold_path) if f.endswith('.wav')]\n        total_files += len(wav_files)\n        print(f\"   {fold}: {len(wav_files)} files\")\n    \n    print(f\"\\nTotal audio files: {total_files}\")\n    \n    if total_files == 8732:\n        print(\"All 8,732 files present\")\n    else:\n        print(f\"Warning: Expected 8,732, found {total_files}\")\nelse:\n    print(f\"ERROR: Dataset not found at {dataset_path}\")\n    print(\"Please ensure you added the urbansound8k dataset to your notebook\")\n\n# Check metadata\nif os.path.exists(metadata_path):\n    print(f\"\\nMetadata file found: UrbanSound8K.csv\")\nelse:\n    print(f\"\\nERROR: Metadata not found at {metadata_path}\")\n\nprint(\"\\n\" + \"=\"*70)\nif os.path.exists(dataset_path) and os.path.exists(metadata_path):\n    print(\"DATASET VERIFIED AND READY!\")\nelse:\n    print(\"DATASET SETUP INCOMPLETE - Check errors above\")\nprint(\"=\"*70)","metadata":{"id":"nWuAHEEFBUtm","executionInfo":{"status":"ok","timestamp":1761307077035,"user_tz":-120,"elapsed":137,"user":{"displayName":"Lutho Mngqibisa","userId":"08014628584478494004"}},"outputId":"5ada171b-40b7-49e6-eaec-5b38631c7bc1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### LOADING METADATA","metadata":{}},{"cell_type":"code","source":"# Load metadata CSV\nmetadata = pd.read_csv(CONFIG['metadata_path'])\n\nprint(f\"\\nMetadata shape: {metadata.shape}\")\nprint(f\"Total samples: {len(metadata)}\")\n\nprint(\"\\nFirst 5 rows:\")\nprint(metadata.head())\n\nprint(\"\\nColumn names:\")\nprint(metadata.columns.tolist())\n\nprint(\"\\nColumn data types:\")\nprint(metadata.dtypes)\n\nprint(\"\\nClass names in dataset:\")\nprint(metadata['class'].unique())\n\n# Store in CONFIG for later use\nCONFIG['metadata'] = metadata\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Metadata loaded successfully!\")\nprint(\"=\"*70)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### DATASET STATISTICS","metadata":{}},{"cell_type":"code","source":"# Basic statistics\nprint(f\"\\nTotal number of audio samples: {len(metadata)}\")\nprint(f\"Number of classes: {metadata['classID'].nunique()}\")\nprint(f\"Number of folds: {metadata['fold'].nunique()}\")\n\n# Class distribution\nprint(\"\\nClass Distribution:\")\nprint(\"-\"*70)\nclass_counts = metadata['class'].value_counts().sort_index()\nfor class_name, count in class_counts.items():\n    percentage = (count / len(metadata)) * 100\n    print(f\"   {class_name:20s}: {count:4d} samples ({percentage:5.2f}%)\")\n\nprint(\"-\"*70)\nprint(f\"   {'TOTAL':20s}: {len(metadata):4d} samples (100.00%)\")\n\n# Check for class imbalance\nmax_count = class_counts.max()\nmin_count = class_counts.min()\nimbalance_ratio = max_count / min_count\nprint(f\"\\nClass Imbalance Ratio: {imbalance_ratio:.2f}\")\nif imbalance_ratio < 1.5:\n    print(\"Dataset is relatively balanced (ratio < 1.5)\")\nelif imbalance_ratio < 2.0:\n    print(\"Dataset shows moderate imbalance (ratio < 2.0)\")\nelse:\n    print(\"Dataset shows significant imbalance (ratio >= 2.0)\")\n\n# Fold distribution\nprint(\"\\nSamples per fold:\")\nfold_counts = metadata['fold'].value_counts().sort_index()\nfor fold, count in fold_counts.items():\n    print(f\"   fold{fold}: {count} samples\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### VISUALIZING CLASS DISTRIBUTION","metadata":{}},{"cell_type":"code","source":"# Create figure\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Get class counts\nclass_counts = metadata['class'].value_counts().sort_index()\n\n# Create bar plot\nbars = ax.bar(range(len(class_counts)), class_counts.values, color='steelblue', alpha=0.8)\n\n# Customize plot\nax.set_xlabel('Class Name', fontsize=12, fontweight='bold')\nax.set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\nax.set_title('Urban Sound 8K - Class Distribution', fontsize=14, fontweight='bold')\nax.set_xticks(range(len(class_counts)))\nax.set_xticklabels(class_counts.index, rotation=45, ha='right')\nax.grid(axis='y', alpha=0.3)\n\n# Add value labels on bars\nfor bar in bars:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{int(height)}',\n            ha='center', va='bottom', fontsize=10)\n\n# Add horizontal line for mean\nmean_samples = class_counts.mean()\nax.axhline(y=mean_samples, color='red', linestyle='--', alpha=0.5, label=f'Mean: {mean_samples:.0f}')\nax.legend()\n\nplt.tight_layout()\n\n# Save figure\nfig_path = f\"{CONFIG['figures_dir']}/class_distribution.png\"\nplt.savefig(fig_path, dpi=300, bbox_inches='tight')\nprint(f\"\\nFigure saved to: {fig_path}\")\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SELECTING SAMPLE AUDIO FILES","metadata":{}},{"cell_type":"code","source":"# Select one sample from each class\nsample_files = []\nfor class_name in CONFIG['class_names']:\n    # Get first sample of this class\n    class_samples = metadata[metadata['class'] == class_name]\n    if len(class_samples) > 0:\n        sample = class_samples.iloc[0]\n        sample_files.append(sample)\n\nprint(f\"\\nSelected {len(sample_files)} sample audio files (one per class)\")\nprint(\"\\nSample details:\")\nprint(\"-\"*70)\n\nfor i, sample in enumerate(sample_files):\n    fold = sample['fold']\n    filename = sample['slice_file_name']\n    class_name = sample['class']\n    class_id = sample['classID']\n    \n    # Build full path\n    audio_path = f\"{CONFIG['audio_dir']}/fold{fold}/{filename}\"\n    \n    # Verify file exists\n    exists = os.path.exists(audio_path)\n    status = \"OK\" if exists else \"NOT FOUND\"\n    \n    print(f\"{i+1:2d}. Class: {class_name:20s} | ID: {class_id} | File: {filename:30s} | {status}\")\n\nprint(\"-\"*70)\nprint(f\"All sample files verified!\")\nprint(\"=\"*70)\n\n# Store samples for later use\nCONFIG['sample_files'] = sample_files","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### VISUALIZING SAMPLE WAVEFORMS","metadata":{}},{"cell_type":"code","source":"# Create figure with subplots\nfig, axes = plt.subplots(5, 2, figsize=(15, 12))\naxes = axes.ravel()\n\nprint(\"\\nGenerating waveform plots for all 10 classes...\")\nprint(\"This may take 1-2 minutes...\")\n\nfor i, sample in enumerate(CONFIG['sample_files']):\n    fold = sample['fold']\n    filename = sample['slice_file_name']\n    class_name = sample['class']\n    audio_path = f\"{CONFIG['audio_dir']}/fold{fold}/{filename}\"\n    \n    # Load audio\n    signal, sr = librosa.load(audio_path, sr=CONFIG['sample_rate'], duration=CONFIG['duration'])\n    \n    # Create time axis\n    time = np.linspace(0, len(signal)/sr, len(signal))\n    \n    # Plot waveform\n    axes[i].plot(time, signal, linewidth=0.5, color='blue', alpha=0.7)\n    axes[i].set_title(f\"{i+1}. {class_name}\", fontsize=10, fontweight='bold')\n    axes[i].set_xlabel('Time (s)', fontsize=8)\n    axes[i].set_ylabel('Amplitude', fontsize=8)\n    axes[i].grid(alpha=0.3)\n    axes[i].set_xlim([0, CONFIG['duration']])\n\nplt.tight_layout()\n\n# Save figure\nfig_path = f\"{CONFIG['figures_dir']}/sample_waveforms.png\"\nplt.savefig(fig_path, dpi=300, bbox_inches='tight')\nprint(f\"\\nFigure saved to: {fig_path}\")\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### AUDIO PLAYBACK","metadata":{}},{"cell_type":"code","source":"# Play samples\nfor i, sample in enumerate(CONFIG['sample_files']):\n    fold = sample['fold']\n    filename = sample['slice_file_name']\n    class_name = sample['class']\n    audio_path = f\"{CONFIG['audio_dir']}/fold{fold}/{filename}\"\n    \n    print(f\"\\n{i+1}. Class: {class_name}\")\n    print(f\"   File: {filename}\")\n    display(ipd.Audio(audio_path))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n # Feature Extraction\n ___","metadata":{}},{"cell_type":"markdown","source":"### EXTRACTING MFCC FEATURES FROM ALL AUDIO FILES","metadata":{}},{"cell_type":"code","source":"# Prepare containers\nfeatures = []\nlabels = []\n\nprint(f\"\\nExtracting MFCC features (n_mfcc={CONFIG['n_mfcc']})\")\nprint(f\"Processing {len(metadata)} audio files...\")\nprint(\"This will take 10-15 minutes\")\n\n# Loop through all audio files\nfor idx, row in tqdm(metadata.iterrows(), total=len(metadata)):\n    fold = row['fold']\n    filename = row['slice_file_name']\n    class_id = row['classID']\n    \n    # Build audio path\n    audio_path = f\"{CONFIG['audio_dir']}/fold{fold}/{filename}\"\n    \n    try:\n        # Load audio\n        signal, sr = librosa.load(audio_path, sr=CONFIG['sample_rate'], duration=CONFIG['duration'])\n        \n        # Extract MFCC\n        mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=CONFIG['n_mfcc'])\n        \n        # Average across time to get (120,) features\n        mfcc_mean = np.mean(mfcc, axis=1)\n        \n        features.append(mfcc_mean)\n        labels.append(class_id)\n        \n    except Exception as e:\n        print(f\"\\nError processing {filename}: {e}\")\n        continue\n\n# Convert to numpy arrays\nfeatures = np.array(features)\nlabels = np.array(labels)\n\nprint(\"\\n\" + \"-\"*70)\nprint(f\"Feature extraction complete!\")\nprint(f\"Features shape: {features.shape}\")\nprint(f\"Labels shape: {labels.shape}\")\n\n# Save features\nfeatures_path = f\"{CONFIG['project_dir']}/features.npy\"\nlabels_path = f\"{CONFIG['project_dir']}/labels.npy\"\n\nnp.save(features_path, features)\nnp.save(labels_path, labels)\n\nprint(f\"\\nFeatures saved to: {features_path}\")\nprint(f\"Labels saved to: {labels_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ORGANIZING DATA BY FOLDS FOR 10-FOLD CROSS-VALIDATION","metadata":{}},{"cell_type":"code","source":"# Map each sample to its fold using metadata\nfold_mapping = {}\nfor idx, row in metadata.iterrows():\n    fold_mapping[idx] = row['fold']\n\n# Create fold-organized data structure\nfolds_data = {i: {'indices': [], 'features': [], 'labels': []} for i in range(1, 11)}\n\nfor idx in range(len(features)):\n    fold_num = fold_mapping[idx]\n    folds_data[fold_num]['indices'].append(idx)\n    folds_data[fold_num]['features'].append(features[idx])\n    folds_data[fold_num]['labels'].append(labels[idx])\n\n# Convert to numpy arrays\nfor fold_num in range(1, 11):\n    folds_data[fold_num]['features'] = np.array(folds_data[fold_num]['features'])\n    folds_data[fold_num]['labels'] = np.array(folds_data[fold_num]['labels'])\n\n# Display fold sizes\nprint(\"\\nSamples per fold:\")\nprint(\"-\"*70)\nfor fold_num in range(1, 11):\n    n_samples = len(folds_data[fold_num]['features'])\n    print(f\"   Fold {fold_num:2d}: {n_samples:4d} samples\")\nprint(\"-\"*70)\nprint(f\"   Total:  {sum(len(folds_data[i]['features']) for i in range(1, 11)):4d} samples\")\n\n# Store in CONFIG\nCONFIG['folds_data'] = folds_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### CREATING FOLD SPLIT FUNCTION","metadata":{}},{"cell_type":"code","source":"def get_fold_split(test_fold, val_fold):\n    \"\"\"\n    Get train/val/test split for a given test and validation fold.\n    \n    Args:\n        test_fold: Fold number to use as test set (1-10)\n        val_fold: Fold number to use as validation set (1-10, must be != test_fold)\n    \n    Returns:\n        X_train, X_val, X_test, y_train, y_val, y_test, scaler\n    \"\"\"\n    assert test_fold != val_fold, \"Test and validation folds must be different!\"\n    assert 1 <= test_fold <= 10, \"Test fold must be 1-10\"\n    assert 1 <= val_fold <= 10, \"Validation fold must be 1-10\"\n    \n    # Get test data\n    X_test = folds_data[test_fold]['features']\n    y_test = folds_data[test_fold]['labels']\n    \n    # Get validation data\n    X_val = folds_data[val_fold]['features']\n    y_val = folds_data[val_fold]['labels']\n    \n    # Get training data (all other folds)\n    X_train_list = []\n    y_train_list = []\n    \n    for fold_num in range(1, 11):\n        if fold_num != test_fold and fold_num != val_fold:\n            X_train_list.append(folds_data[fold_num]['features'])\n            y_train_list.append(folds_data[fold_num]['labels'])\n    \n    X_train = np.vstack(X_train_list)\n    y_train = np.concatenate(y_train_list)\n    \n    # Normalize features \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n    \n    return X_train, X_val, X_test, y_train, y_val, y_test, scaler\n\n# Test the function\nprint(\"\\nTesting fold split function...\")\nX_train, X_val, X_test, y_train, y_val, y_test, scaler = get_fold_split(test_fold=10, val_fold=9)\n\nprint(f\"\\nExample split (test=fold10, val=fold9):\")\nprint(f\"   Training samples:   {len(X_train)}\")\nprint(f\"   Validation samples: {len(X_val)}\")\nprint(f\"   Test samples:       {len(X_test)}\")\nprint(f\"   Total:              {len(X_train) + len(X_val) + len(X_test)}\")\n\n# Store function in CONFIG\nCONFIG['get_fold_split'] = get_fold_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n# Baseline Model (NAIVE BAYES) \n---","metadata":{}},{"cell_type":"markdown","source":"### TRAINING NAIVE BAYES BASELINE - 10-FOLD CROSS-VALIDATION","metadata":{}},{"cell_type":"code","source":"# Store results for each fold\nbaseline_results = {\n    'fold_accuracies': [],\n    'fold_times': [],\n    'fold_details': []\n}\n\nprint(\"\\nRunning 10-fold cross-validation...\")\nprint(\"This will take ~5-10 minutes\")\nprint(\"-\"*70)\n\n# Loop through all 10 folds\nfor test_fold in range(1, 11):\n    # Use next fold as validation (wrap around)\n    val_fold = (test_fold % 10) + 1\n    \n    print(f\"\\nFold {test_fold}/10: Test=fold{test_fold}, Val=fold{val_fold}\")\n    \n    # Get data split\n    X_train, X_val, X_test, y_train, y_val, y_test, scaler = get_fold_split(test_fold, val_fold)\n    \n    # Train model\n    nb_model = GaussianNB()\n    start_time = time.time()\n    nb_model.fit(X_train, y_train)\n    train_time = time.time() - start_time\n    \n    # Predict on validation\n    y_pred_val = nb_model.predict(X_val)\n    val_accuracy = accuracy_score(y_val, y_pred_val)\n    \n    # Predict on test\n    y_pred_test = nb_model.predict(X_test)\n    test_accuracy = accuracy_score(y_test, y_pred_test)\n    \n    # Calculate detailed metrics\n    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_test, average='macro')\n    \n    # Store results\n    baseline_results['fold_accuracies'].append(test_accuracy)\n    baseline_results['fold_times'].append(train_time)\n    baseline_results['fold_details'].append({\n        'test_fold': test_fold,\n        'val_fold': val_fold,\n        'val_accuracy': val_accuracy,\n        'test_accuracy': test_accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'train_time': train_time\n    })\n    \n    print(f\"   Val Acc: {val_accuracy*100:.2f}% | Test Acc: {test_accuracy*100:.2f}% | Time: {train_time:.2f}s\")\n\n# Calculate aggregate statistics\nmean_accuracy = np.mean(baseline_results['fold_accuracies'])\nstd_accuracy = np.std(baseline_results['fold_accuracies'])\ntotal_time = sum(baseline_results['fold_times'])\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"BASELINE RESULTS (10-FOLD CROSS-VALIDATION)\")\nprint(\"=\"*70)\nprint(f\"\\nTest Accuracy: {mean_accuracy*100:.2f}% Â± {std_accuracy*100:.2f}%\")\nprint(f\"Total training time: {total_time:.2f} seconds\")\nprint(f\"\\nPer-fold accuracies:\")\nfor i, acc in enumerate(baseline_results['fold_accuracies'], 1):\n    print(f\"   Fold {i:2d}: {acc*100:.2f}%\")\n\n# Store in CONFIG\nCONFIG['baseline_results'] = baseline_results\nCONFIG['baseline_mean_accuracy'] = mean_accuracy\nCONFIG['baseline_std_accuracy'] = std_accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SAVING BASELINE RESULTS","metadata":{}},{"cell_type":"code","source":"# Prepare results for saving (convert numpy types to python types)\nbaseline_summary = {\n    'model': 'Naive Bayes (GaussianNB)',\n    'evaluation': '10-Fold Cross-Validation',\n    'mean_accuracy': float(mean_accuracy),\n    'std_accuracy': float(std_accuracy),\n    'min_accuracy': float(min(baseline_results['fold_accuracies'])),\n    'max_accuracy': float(max(baseline_results['fold_accuracies'])),\n    'total_training_time': float(total_time),\n    'fold_results': [\n        {\n            'test_fold': d['test_fold'],\n            'val_fold': d['val_fold'],\n            'test_accuracy': float(d['test_accuracy']),\n            'val_accuracy': float(d['val_accuracy']),\n            'precision': float(d['precision']),\n            'recall': float(d['recall']),\n            'f1': float(d['f1'])\n        }\n        for d in baseline_results['fold_details']\n    ]\n}\n\n# Save\nresults_path = f\"{CONFIG['results_dir']}/baseline_results_10fold.json\"\nwith open(results_path, 'w') as f:\n    json.dump(baseline_summary, f, indent=4)\n\nprint(f\"\\nBaseline results saved to: {results_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### VISUALIZING BASELINE RESULTS","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12, 6))\n\n# Plot fold accuracies\nfolds = list(range(1, 11))\naccuracies = [d['test_accuracy']*100 for d in baseline_results['fold_details']]\n\nbars = ax.bar(folds, accuracies, color='steelblue', alpha=0.8)\n\n# Add mean line\nax.axhline(y=mean_accuracy*100, color='red', linestyle='--', linewidth=2, \n           label=f'Mean: {mean_accuracy*100:.2f}%')\n\n# Add std bands\nax.axhspan(mean_accuracy*100 - std_accuracy*100, \n           mean_accuracy*100 + std_accuracy*100, \n           alpha=0.2, color='red', label=f'Â±1 Std: {std_accuracy*100:.2f}%')\n\n# Customize\nax.set_xlabel('Test Fold', fontsize=12, fontweight='bold')\nax.set_ylabel('Test Accuracy (%)', fontsize=12, fontweight='bold')\nax.set_title('Baseline (Naive Bayes) - 10-Fold Cross-Validation Results', \n             fontsize=14, fontweight='bold')\nax.set_xticks(folds)\nax.grid(axis='y', alpha=0.3)\nax.legend()\n\n# Add values on bars\nfor bar, acc in zip(bars, accuracies):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{acc:.1f}%', ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\n\n# Save\nfig_path = f\"{CONFIG['figures_dir']}/baseline_10fold_results.png\"\nplt.savefig(fig_path, dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nFigure saved to: {fig_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}